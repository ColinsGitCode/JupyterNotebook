{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-04T10:13:23.191483Z",
     "start_time": "2017-11-04T10:13:19.377571Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 常用的引用和创建对象实体\n",
    "from pyspark import SparkConf,SparkContext\n",
    "conf = SparkConf().setMaster(\"local\").setAppName(\"My App\")\n",
    "sc = SparkContext(conf = conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1, Motivation\n",
    "\n",
    "# 2, Creating Pair RDDs\n",
    "\n",
    "Always creat Pair RDDs by using RDD.map()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-04T10:13:25.582784Z",
     "start_time": "2017-11-04T10:13:23.192833Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('hello', 'hello world'), ('hi', 'hi Worriors'), ('for', 'for the horde, bool and lighting!!!')]\n"
     ]
    }
   ],
   "source": [
    "# Examples\n",
    "lines = sc.parallelize([\"hello world\",\"hi Worriors\",\"for the horde, bool and lighting!!!\"])\n",
    "# create a pair RDD by using the first word as the KEY\n",
    "pairs = lines.map(lambda x: (x.split(\" \")[0],x))\n",
    "print(pairs.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3, Transformations on Pair RDDs\n",
    "\n",
    "Pairs RDDs are allowed to use all the transformations available to standard RDDs. Since Pair RDDs contains tuples, we need to pass functions that operate on tuples rather than on individual elements.\n",
    "\n",
    "Pair RDDs are also still RDDs, and thus support the same functions as RDDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-04T10:13:25.708619Z",
     "start_time": "2017-11-04T10:13:25.584664Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('hello', 'hello world'), ('hi', 'hi Worriors')]\n"
     ]
    }
   ],
   "source": [
    "# filter the second elements of lines\n",
    "result = pairs.filter(lambda keyValue: len(keyValue[1]) < 20)\n",
    "print(result.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Aggregations\n",
    "\n",
    "### Functions:\n",
    "1. reduceByKey():**similar with reduce()**, runs several parallel reduce operations, one for each key in the dataset, where each operation combines values that have the same key. It returns a new RDD consisting of each key and the reduced value for that key.\n",
    "2. foldByKey(): **similar with fold()**\n",
    "3. mapValues(): Usecase in the picture below\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/ColinsGitCode/JupyterNotebook/c0ba7cc4ceb1651a33bc65f0f3c738d68abbf25f/ipynbFiles/Materials/Usecase1.jpg\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-04T10:13:26.885545Z",
     "start_time": "2017-11-04T10:13:25.718956Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['panda', 'pink', 'pirate', 'panda', 'pink']\n",
      "[0, 3, 3, 1, 4]\n",
      "[(0, 1), (3, 1), (3, 1), (1, 1), (4, 1)]\n",
      "[(1, 2), (7, 2), (3, 1)]\n",
      "[('panda', (1, 2)), ('pink', (7, 2)), ('pirate', (3, 1))]\n"
     ]
    }
   ],
   "source": [
    "# examples of picture\n",
    "# How create a Pais RDD\n",
    "word = [('panda',0),('pink',3),('pirate',3),('panda',1),('pink',4)] # List insided with tuples\n",
    "Words = sc.parallelize(word)\n",
    "print(Words.keys().collect())\n",
    "print(Words.values().collect())\n",
    "new_words = Words.mapValues(lambda x:(x,1))\n",
    "print(new_words.values().collect())\n",
    "NewWords = new_words.reduceByKey(lambda x,y:(x[0] + y[0], x[1] + y[1]))\n",
    "print(NewWords.values().collect())\n",
    "print(NewWords.collect())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-04T10:13:27.933221Z",
     "start_time": "2017-11-04T10:13:26.897701Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '#', 'see', '/usr/share/doc/bash/examples/startup-files', '(in', 'the', 'package', 'bash-doc)', '#', 'for']\n",
      "[('', 598), ('#', 73), ('see', 3), ('/usr/share/doc/bash/examples/startup-files', 1), ('(in', 1), ('the', 27), ('package', 1), ('bash-doc)', 1), ('for', 9), ('examples', 1)]\n",
      "<class 'collections.defaultdict'>\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# Example of WordCount of a file(total number of each word)\n",
    "File_RDD = sc.textFile(\"/home/colin/bashrc\")\n",
    "Txt = File_RDD.flatMap(lambda x:x.split(\" \")) # this step is important!!!\n",
    "print(Txt.take(10))\n",
    "result = Txt.map(lambda x:(x,1)).reduceByKey(lambda x,y: x + y)\n",
    "print(result.take(10))\n",
    "Res = Txt.countByValue() # countByValue() count each word, and return a collections.defaultdict\n",
    "print(type(Res))\n",
    "print(Res[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CombineByKey() : the most general of the per-key aggreation functions\n",
    "\n",
    "**Contains:**\n",
    "1. createCombiner(): Called when meets a element which has a new key, to create the initial value for the accumulator on that key\n",
    "2. mergeValue(): Called when meets a element which do not has a new key, to merge the values\n",
    "3. mergeCombiners(): Called when has 2 or more accumulator for one key, do the values merge for the same keys\n",
    "\n",
    "**Parameters**: has many parameters to control the steps of the aggreation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-04T11:44:53.340998Z",
     "start_time": "2017-11-04T11:44:53.083784Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('panda', 0), ('pink', 3), ('pirate', 3), ('panda', 1), ('pink', 4)]\n",
      "[('panda', (1, 2)), ('pink', (7, 2)), ('pirate', (3, 1))]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('panda', 0.5), ('pink', 3.5), ('pirate', 3.0)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example: Using combineByKey() tp obtain the averages of each key\n",
    "print(Words.collect()) # ---> (key,value)\n",
    "sumCount = Words.combineByKey((lambda x:(x,1)), # Each Element: each value change into (value,1), ---> (key,(value,1))\n",
    "                              (lambda x,y:(x[0] + y, x[1] + 1)), # Each Partition: When same key: merge the same key values, count the number of same key values,--->(key,(TotalValues,CountNumber)) \n",
    "                              (lambda x,y:(x[0] + y[0], x[1] + y[1]))) # Whole RDD: --->(key,(TotalValueWholeRDD,TotalCountNumberWholeRDD))\n",
    "print(sumCount.collect())\n",
    "\n",
    "# mapAverage = sumCount.map(lambda key,xy:(key,xy[0]/xy[1])) \n",
    "# There is a Error when executed last one sentences,cased by Tuple Parameter Unpacking which has been removed in Python\n",
    "# the Error can be slove by the webpage : https://stackoverflow.com/questions/40207441/python-spark-combinebykey-average\n",
    "def pri(key_vals): \n",
    "    #print(\"Key is %s, Values is %d, Count is %d, Average is %f\" %(key,val[0],val[1],val[0]/val[1]))\n",
    "    (key,(total,count)) = key_vals\n",
    "    return key,(total/count)\n",
    "sumCount.map(pri).collect()\n",
    "\n",
    "#mapAverage.collectAsMap()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** *Examples: Using combineByKey() to obtain the averages of each key* **\n",
    "```\n",
    "sumCount = nums.combineBykey((lambda x: (x,1)),\n",
    "                             (lambda x,y:(\n",
    "   \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "205px",
    "left": "576px",
    "right": "20px",
    "top": "112px",
    "width": "566px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
