{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-24T12:31:39.322427Z",
     "start_time": "2017-11-24T12:31:39.030714Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 常用的引用和创建对象实体\n",
    "from pyspark import SparkConf,SparkContext\n",
    "# conf = SparkConf().setMaster(\"local\").setAppName(\"My App\")\n",
    "# sc = SparkContext(conf = conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1, 使用SparkConf配置Spark\n",
    "- 对Spark的性能调优，通常就是修改Spark应用的运行时配置选项。\n",
    "- Spark中最主要的配置机制是通过SparkConf类对Spark进行配置的\n",
    "- **当创建出一个SparkContext时，就需要创建出一个SparkConf实例**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-24T12:35:59.050210Z",
     "start_time": "2017-11-24T12:35:59.035993Z"
    }
   },
   "source": [
    "```\n",
    "# 创建一个conf对象, 目前错误\n",
    "conf = new SparkConf()\n",
    "conf.set(\"spark.app,name\", \"My Spark App\")\n",
    "conf.set(\"saprk.master\", \"local[4]\")\n",
    "# .....\n",
    "# 还可以配置很多\n",
    "\n",
    "# 使用这个配置创建一个SparkContext对象\n",
    "sc= SparkContext(conf)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- SparkConf实例包含用户要重载的配置选项的键值对\n",
    "- Spark中的每个配置选项都是基于字符串形式的键值对\n",
    "- 要使用创建出来的SparkConf对象可以调用**set()**方法来添加配置项的设置，也可以使用一些设置好的函数如：\n",
    "     - **setAppName(), setMaster()**等函数\n",
    "- **Spark配置的优先级**\n",
    "    1. 程序中显示声明的**set()**方式具有最高的优先级\n",
    "    2. spark-submit 提交的参数具有第二优先级\n",
    "    3. 在配置文件中“conf/spark-env.sh”具有第三优先级\n",
    "    4，spark默认的配置具有最低的优先级\n",
    "- **几乎所有的Spark配置都发生在SparkConf的创建过程中，但是有一个例外**\n",
    "    1. conf/spark-env.sh文件中的环境变量 **SPARK_LOCAL_DIRS** \n",
    "    2. 它需要被设定为用逗号隔开的存储位置列表来指定Spark用来混洗数据的本地存储路径\n",
    "    3. 在**standalone**he**Mesos**情况下需要配置好\n",
    "    4. 之所以需要特别设置，是因为它的值在胡同的物理主机上会有区别\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2, Spark执行的组成部分：作业job，任务task，步骤stage\n",
    "- DAG\n",
    "- 每个RDD维护了其指向一个或多个父节点的引用，以及表示其与父节点之间关系的信息\n",
    "- 可以**RDD.toDebugString()**方法来查看RDD的谱系\n",
    "- 步骤（stage）, 作业（job）, 任务（Task）\n",
    "- 由特定**Actions**所生成的步骤**stage**的集合被称为一个作业**job**\n",
    "- **一旦步骤图被确定下来，任务就会被创建出来并发给内部的调度器（scheduler）**\n",
    "- **一个物理步骤会启动很多任务**\n",
    "- **每个任务都是在不同的数据分区上做同样的事情**\n",
    "- **任务内的流程如下：**\n",
    "   1. 从数据存储（如果该RDD是一个输入RDD）或已有RDD(如果该步骤是基于已经缓存的数据）或**数据混洗**的输出中获得输入数据\n",
    "   2. 执行必要的操作来计算出这些操作所代表的RDD. 例如，map(),filter（）操作\n",
    "   3. 把输出写到一个数据混洗的文件中，写入外部存储或者发回**Driver**\n",
    "- **Spark的大部分日志信息和工具都是以步骤，任务或数据混洗为单位的**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Spark执行流程 (在一个app中，会产生很多RDD，此过程会执行很多次)\n",
    "1. 用户代码定义RDD的DAG\n",
    "   - RDD操作会创建出新的RDD，并引用它们的父节点，这样就创建出一个DAG\n",
    "2. **Actions**强制把DAG转译为执行计划\n",
    "   - 当执行一个**Actions**,就必须计算该RDD的父节点。\n",
    "   - Spark **Scheduler** 提交一个 **Job** 来计算所有必要的RDD\n",
    "   - 此 **Job** 包含一个或多个 **stage** , 每个 **stage** 就是一批**并行执行**的计算任务 **Task** , **每个 Task 做同样的工作**\n",
    "   - 一个 **stage** 对应 DAG 中的一个或多个 RDD\n",
    "   - 一个 **stage** 对应多个 RDD 是以为发生了 流水线执行（**pipelining**: 把多个RDD合并到一个 stage 中）\n",
    "       - ** 当RDD不需要数据混洗就能从父节点计算出来时，Spark scheduler 会自动执行 pipelining**\n",
    "3. Task 于 cluster 中调度并执行\n",
    "   - **stage** 是按顺序处理的，**task** 则独立的启动来计算出 RDD 的一部分\n",
    "   - 一旦 **Job** 的最后一个 **stage** 完成了， 一个 **Action** 也就完成了"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 3查找信息\n",
    "- 应用执行时的进度信息和性能指标：\n",
    "   1. Spark的网页用户界面\n",
    "   2. 驱动器进程和执行器进程生成的日志文件\n",
    "# 3.1 Spark网页用户界面\n",
    "- Page --> 131 to 133"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4, 关键性能考量\n",
    "# 4.1 并行度\n",
    "- RDD的逻辑表示其实是一个对象集合\n",
    "- **在 Spark 的物理执行期间**\n",
    "   1. RDD会被分成一系列的分区，每个分区都是整个RDD数据的子集\n",
    "   2. 当Spark调度 并运行任务时，Spark会为每个分区中的数据创建出一个任务。\n",
    "       - **每个分区对应一个任务**\n",
    "   3. **该任务在默认情况下，会需要集群中的一个计算核心来执行**\n",
    "   4. Spark会针对RDD直接自动推断出合适的并行度，对于大部分用例来说已经足够了\n",
    "   5. 输入RDD的并行度一般由其底层的存储结构来决定\n",
    "- **并行度如何影响性能**\n",
    "   1. 并行度过低，会导致集群资源的浪费\n",
    "   2. 并行度过高，每个分区产生的间接开销累计起来就会更大\n",
    "      - 评价标准：任务几乎在毫秒级完成，或者没有读写任何数据\n",
    "\n",
    "- **两种方式对并行度进行调优**\n",
    "   1. 在数据混洗操作后，使用参数的方式为混洗后的RDD指定并行度\n",
    "   2. 对于任何已经存在的RDD,可以进行重新分区来获得更多或更少的分区\n",
    "       - 重新分区可以通过 **repartition()**进行实现，该操作会把RDD随机打乱并分成设定的分区数目。\n",
    "       - **如果要减少分区数，可以使用 coalesce() ，它没有打乱数据，比 repartition()高效**\n",
    "       \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 序列化格式\n",
    "- 当Spark需要使用网络传输数据，或是将数据写到磁盘上的时候，Spark需要把数据序列化为二进制格式\n",
    "- **序列化会在数据进行混洗操作时发生，此时有可能需要通过网络传输大量数据**\n",
    "- 默认情况下，Spark会使用Java内建的序列化库\n",
    "- **Spark也支持第三方序列化库 Kryo, 提供更快，更高压缩比的二进制表示，但是不能序列化全部类型的对象**\n",
    "    1. [Kryo@github](http://github.com/EsotericSoftware/kryo)\n",
    "    2. **几乎所有的应用都在迁移到kryo之后获得了更好性能**\n",
    "    3. 可以自己配置Spark使用Kryo作为序列化工具（详细方法需要在有需要时自己学习）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 内存管理\n",
    "- 理解并调优Spark的内存使用方法可以帮助优化Spark的应用\n",
    "- 在每个**Executor**中，内存有以下用途：\n",
    "   1. RDD存储\n",
    "     - 当调用RDD的**persist()**和**cache()**方法时，这个RDD的分区会被存储到缓存区中\n",
    "     - Spark会根据**spark.storage.memoryFraction**限制用来缓存的内存占整个JVM堆空间的比例大小。\n",
    "     - **如果超出限制，旧的分区会被移除内存**\n",
    "   2. 数据混洗与聚合的缓存区\n",
    "     - 进行混洗操作的时候，Spark会创建出一些中间缓存区来存储数据混洗的输出数据\n",
    "     - 这些缓存区用来存储聚合操作的中间结果，以及数据混洗操作中直接输出的部分缓存数据。\n",
    "     - Spark会尝试根据**spark,shuffle.memoryFraction**限定这种缓存区占总内存的比例\n",
    "   3. 用户代码\n",
    "     - spark可以执行任意的用户代码，所以用户的函数可以自行申请大量的内存\n",
    "     - 用户代码可以访问JVM堆空间中出分配给RDD存储和数据混洗存储以外的全部剩余空间\n",
    "- 默认情况下：60%给RDD存储，20%留给数据混洗产生的数据，20%留给用户的代码\n",
    "- 可以自行配置适合的内存分配方案\n",
    "- **Tips**\n",
    "    1. 还可以为一些工作负载改进缓存行为的某些要素\n",
    "       - Spark默认的**cache()**操作会以**MEMORY_ONLY**的存储等级来持久化数据\n",
    "       - 这种模式下，新RDD的分区存储空间不足时，旧的RDD分区就会被删除，再用到这些分区数据的时候需要重新计算\n",
    "       - 有时以**MEMORY_AND_DISK**的存储等级来调用**persist()**会获得更好的效果\n",
    "       - 这种模式下，内存中放不下的旧分区就会被写入磁盘，当再次调用的时候，再从磁盘读取回来\n",
    "       - 这种情况有时比重算分区要低，也更稳定，尤其是在重算代价很大的时候\n",
    "    2. 缓存序列化后的对象而非直接缓存\n",
    "       - 可以设置**MEMORY_ONLY_SER**和**MEMORY_AND_DISK_SER**的存储等级\n",
    "       - 缓存序列化之后的对象可能会是缓存操作变慢，但是可以显著减少JVM的垃圾回收时间\n",
    "       -**这个垃圾回收有缓存的对象数量和大小决定，可以在用户界面中每个任务的垃圾回收时间信息栏看到**\n",
    "       - 如果需要以对象形式缓存大量的数据（GB级）或者存在长时间的垃圾回收，可以考虑这个设置"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.4 硬件供给\n",
    "- Spark的硬件资源会显著影响应用的完成时间\n",
    "- 影响集群规模的要素：\n",
    "   1. 每个**Executor**的内存大小\n",
    "   2. 每个**Executor**占用的核心数\n",
    "   3. **Executor**的总数\n",
    "   4. 用来存储数据的本地磁盘数量\n",
    "   \n",
    "- 具体供给方案:\n",
    "   1. 各部署模式下，**Executor**的内存都可以通过**spark.executor.memory**配置项或者**spark-submit**的** --executor-memory**标记来设置\n",
    "   2. **Executor**的数目以及**Executor**的核心数的配置选项则取决于不同的部署模式\n",
    "      - **YARN** ： **spark.executor.cores** 或 **--executor-cores** 来设置每个**Executor**的核心数，通过** --num-executors** 设置**Executor**的个数\n",
    "      - **Mesos**和**Standalone** : 会从调度器提供的资源中获取尽可能多的核心以用于**Executor**，不过它们也支持通过设置 **spark.cores.max** 来限制一个应用中所有 **Executor** 所使用的核心的总数\n",
    "      \n",
    "- 一般来说：\n",
    "   1. 更大的内存和更多的计算核心对spark应用更有用处\n",
    "   2. 如果确认要使用缓存，那么内存中缓存的数据越多，应用的表现就会越好\n",
    "   3. 大量使用本地磁盘可以帮助提升Spark应用的性能\n",
    "      - **YARN** : **YARN**有自己的存储机制，Spark会读取**YARN**本身的设置\n",
    "      - **Standalone** : conf/spark-env.sh文件中**SPARK_LOCAL_DIRS**环境变量\n",
    "      - **Mesos** : **spark.local.dir**选项\n",
    "      - **所有情况下，本地目录都应该设置成有逗号隔开的目录列表**，一般，在磁盘的每个分卷中都为spark设置一个本地目录，写操作会被均衡地分配到所有提供的目录中，磁盘越多，可以提高吞吐量\n",
    "      \n",
    "- **切记： 越多越好的原则在设置 Executor 内存时并不一定适用**\n",
    "   1. 使用巨大的堆空间可能会导致：**垃圾回收的长时间暂停**，严重影响spark作业的吞吐量\n",
    "   2. 较小的内存的**Executor**可以缓解这个问题，一般**不超过64GB**\n",
    "   3. **YARN**和**Mesos**本身就支持在一个物理主机上运行多个较小的**Executor**\n",
    "   4. **Standalone**： 需要设置 conf/spark-env.sh 中的 **SPARK_WORKER_INSTANCES** 来设置一台物理主机上运行的 **Executor**的个数（Spark 2.2 貌似已经删除）\n",
    "   5. 序列化存储格式同样可以减轻垃圾回收带来的影响\n",
    "      \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
