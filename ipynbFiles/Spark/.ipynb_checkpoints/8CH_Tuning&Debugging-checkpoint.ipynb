{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-24T12:31:39.322427Z",
     "start_time": "2017-11-24T12:31:39.030714Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 常用的引用和创建对象实体\n",
    "from pyspark import SparkConf,SparkContext\n",
    "# conf = SparkConf().setMaster(\"local\").setAppName(\"My App\")\n",
    "# sc = SparkContext(conf = conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1, 使用SparkConf配置Spark\n",
    "- 对Spark的性能调优，通常就是修改Spark应用的运行时配置选项。\n",
    "- Spark中最主要的配置机制是通过SparkConf类对Spark进行配置的\n",
    "- **当创建出一个SparkContext时，就需要创建出一个SparkConf实例**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-24T12:35:59.050210Z",
     "start_time": "2017-11-24T12:35:59.035993Z"
    }
   },
   "source": [
    "```\n",
    "# 创建一个conf对象, 目前错误\n",
    "conf = new SparkConf()\n",
    "conf.set(\"spark.app,name\", \"My Spark App\")\n",
    "conf.set(\"saprk.master\", \"local[4]\")\n",
    "# .....\n",
    "# 还可以配置很多\n",
    "\n",
    "# 使用这个配置创建一个SparkContext对象\n",
    "sc= SparkContext(conf)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- SparkConf实例包含用户要重载的配置选项的键值对\n",
    "- Spark中的每个配置选项都是基于字符串形式的键值对\n",
    "- 要使用创建出来的SparkConf对象可以调用**set()**方法来添加配置项的设置，也可以使用一些设置好的函数如：\n",
    "     - **setAppName(), setMaster()**等函数\n",
    "- **Spark配置的优先级**\n",
    "    1. 程序中显示声明的**set()**方式具有最高的优先级\n",
    "    2. spark-submit 提交的参数具有第二优先级\n",
    "    3. 在配置文件中“conf/spark-env.sh”具有第三优先级\n",
    "    4，spark默认的配置具有最低的优先级\n",
    "- **几乎所有的Spark配置都发生在SparkConf的创建过程中，但是有一个例外**\n",
    "    1. conf/spark-env.sh文件中的环境变量 **SPARK_LOCAL_DIRS** \n",
    "    2. 它需要被设定为用逗号隔开的存储位置列表来指定Spark用来混洗数据的本地存储路径\n",
    "    3. 在**standalone**he**Mesos**情况下需要配置好\n",
    "    4. 之所以需要特别设置，是因为它的值在胡同的物理主机上会有区别\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2, Spark执行的组成部分：作业job，任务task，步骤stage\n",
    "- DAG\n",
    "- 每个RDD维护了其指向一个或多个父节点的引用，以及表示其与父节点之间关系的信息\n",
    "- 可以**RDD.toDebugString()**方法来查看RDD的谱系\n",
    "- 步骤（stage）, 作业（job）, 任务（Task）\n",
    "- 由特定**Actions**所生成的步骤**stage**的集合被称为一个作业**job**\n",
    "- **一旦步骤图被确定下来，任务就会被创建出来并发给内部的调度器（scheduler）**\n",
    "- **一个物理步骤会启动很多任务**\n",
    "- **每个任务都是在不同的数据分区上做同样的事情**\n",
    "- **任务内的流程如下：**\n",
    "   1. 从数据存储（如果该RDD是一个输入RDD）或已有RDD(如果该步骤是基于已经缓存的数据）或**数据混洗**的输出中获得输入数据\n",
    "   2. 执行必要的操作来计算出这些操作所代表的RDD. 例如，map(),filter（）操作\n",
    "   3. 把输出写到一个数据混洗的文件中，写入外部存储或者发回**Driver**\n",
    "- **Spark的大部分日志信息和工具都是以步骤，任务或数据混洗为单位的**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Spark执行流程 (在一个app中，会产生很多RDD，此过程会执行很多次)\n",
    "1. 用户代码定义RDD的DAG\n",
    "   - RDD操作会创建出新的RDD，并引用它们的父节点，这样就创建出一个DAG\n",
    "2. **Actions**强制把DAG转译为执行计划\n",
    "   - 当执行一个**Actions**,就必须计算该RDD的父节点。\n",
    "   - Spark **Scheduler** 提交一个 **Job** 来计算所有必要的RDD\n",
    "   - 此 **Job** 包含一个或多个 **stage** , 每个 **stage** 就是一批**并行执行**的计算任务 **Task** , **每个 Task 做同样的工作**\n",
    "   - 一个 **stage** 对应 DAG 中的一个或多个 RDD\n",
    "   - 一个 **stage** 对应多个 RDD 是以为发生了 流水线执行（**pipelining**: 把多个RDD合并到一个 stage 中）\n",
    "       - ** 当RDD不需要数据混洗就能从父节点计算出来时，Spark scheduler 会自动执行 pipelining**\n",
    "3. Task 于 cluster 中调度并执行\n",
    "   - **stage** 是按顺序处理的，**task** 则独立的启动来计算出 RDD 的一部分\n",
    "   - 一旦 **Job** 的最后一个 **stage** 完成了， 一个 **Action** 也就完成了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
