{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-04T10:13:23.191483Z",
     "start_time": "2017-11-04T10:13:19.377571Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 常用的引用和创建对象实体\n",
    "from pyspark import SparkConf,SparkContext\n",
    "conf = SparkConf().setMaster(\"local\").setAppName(\"My App\")\n",
    "sc = SparkContext(conf = conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1, Motivation\n",
    "\n",
    "# 2, Creating Pair RDDs\n",
    "\n",
    "Always creat Pair RDDs by using RDD.map()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-04T10:13:25.582784Z",
     "start_time": "2017-11-04T10:13:23.192833Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('hello', 'hello world'), ('hi', 'hi Worriors'), ('for', 'for the horde, bool and lighting!!!')]\n"
     ]
    }
   ],
   "source": [
    "# Examples\n",
    "lines = sc.parallelize([\"hello world\",\"hi Worriors\",\"for the horde, bool and lighting!!!\"])\n",
    "# create a pair RDD by using the first word as the KEY\n",
    "pairs = lines.map(lambda x: (x.split(\" \")[0],x))\n",
    "print(pairs.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3, Transformations on Pair RDDs\n",
    "\n",
    "Pairs RDDs are allowed to use all the transformations available to standard RDDs. Since Pair RDDs contains tuples, we need to pass functions that operate on tuples rather than on individual elements.\n",
    "\n",
    "Pair RDDs are also still RDDs, and thus support the same functions as RDDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-04T10:13:25.708619Z",
     "start_time": "2017-11-04T10:13:25.584664Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('hello', 'hello world'), ('hi', 'hi Worriors')]\n"
     ]
    }
   ],
   "source": [
    "# filter the second elements of lines\n",
    "result = pairs.filter(lambda keyValue: len(keyValue[1]) < 20)\n",
    "print(result.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Aggregations\n",
    "\n",
    "### Functions:\n",
    "1. reduceByKey():**similar with reduce()**, runs several parallel reduce operations, one for each key in the dataset, where each operation combines values that have the same key. It returns a new RDD consisting of each key and the reduced value for that key.\n",
    "2. foldByKey(): **similar with fold()**\n",
    "3. mapValues(): Usecase in the picture below\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/ColinsGitCode/JupyterNotebook/c0ba7cc4ceb1651a33bc65f0f3c738d68abbf25f/ipynbFiles/Materials/Usecase1.jpg\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-04T10:13:26.885545Z",
     "start_time": "2017-11-04T10:13:25.718956Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['panda', 'pink', 'pirate', 'panda', 'pink']\n",
      "[0, 3, 3, 1, 4]\n",
      "[(0, 1), (3, 1), (3, 1), (1, 1), (4, 1)]\n",
      "[(1, 2), (7, 2), (3, 1)]\n",
      "[('panda', (1, 2)), ('pink', (7, 2)), ('pirate', (3, 1))]\n"
     ]
    }
   ],
   "source": [
    "# examples of picture\n",
    "# How create a Pais RDD\n",
    "word = [('panda',0),('pink',3),('pirate',3),('panda',1),('pink',4)] # List insided with tuples\n",
    "Words = sc.parallelize(word)\n",
    "print(Words.keys().collect())\n",
    "print(Words.values().collect())\n",
    "new_words = Words.mapValues(lambda x:(x,1))\n",
    "print(new_words.values().collect())\n",
    "NewWords = new_words.reduceByKey(lambda x,y:(x[0] + y[0], x[1] + y[1]))\n",
    "print(NewWords.values().collect())\n",
    "print(NewWords.collect())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-04T10:13:27.933221Z",
     "start_time": "2017-11-04T10:13:26.897701Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '#', 'see', '/usr/share/doc/bash/examples/startup-files', '(in', 'the', 'package', 'bash-doc)', '#', 'for']\n",
      "[('', 598), ('#', 73), ('see', 3), ('/usr/share/doc/bash/examples/startup-files', 1), ('(in', 1), ('the', 27), ('package', 1), ('bash-doc)', 1), ('for', 9), ('examples', 1)]\n",
      "<class 'collections.defaultdict'>\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# Example of WordCount of a file(total number of each word)\n",
    "File_RDD = sc.textFile(\"/home/colin/bashrc\")\n",
    "Txt = File_RDD.flatMap(lambda x:x.split(\" \")) # this step is important!!!\n",
    "print(Txt.take(10))\n",
    "result = Txt.map(lambda x:(x,1)).reduceByKey(lambda x,y: x + y)\n",
    "print(result.take(10))\n",
    "Res = Txt.countByValue() # countByValue() count each word, and return a collections.defaultdict\n",
    "print(type(Res))\n",
    "print(Res[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CombineByKey() : the most general of the per-key aggreation functions\n",
    "\n",
    "**Contains:**\n",
    "1. createCombiner(): Called when meets a element which has a new key, to create the initial value for the accumulator on that key\n",
    "2. mergeValue(): Called when meets a element which do not has a new key, to merge the values\n",
    "3. mergeCombiners(): Called when has 2 or more accumulator for one key, do the values merge for the same keys\n",
    "\n",
    "**Parameters**: has many parameters to control the steps of the aggreation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-04T11:44:53.340998Z",
     "start_time": "2017-11-04T11:44:53.083784Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('panda', 0), ('pink', 3), ('pirate', 3), ('panda', 1), ('pink', 4)]\n",
      "[('panda', (1, 2)), ('pink', (7, 2)), ('pirate', (3, 1))]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('panda', 0.5), ('pink', 3.5), ('pirate', 3.0)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example: Using combineByKey() tp obtain the averages of each key\n",
    "print(Words.collect()) # ---> (key,value)\n",
    "sumCount = Words.combineByKey((lambda x:(x,1)), # Each Element: each value change into (value,1), ---> (key,(value,1))\n",
    "                              (lambda x,y:(x[0] + y, x[1] + 1)), # Each Partition: When same key: merge the same key values, count the number of same key values,--->(key,(TotalValues,CountNumber)) \n",
    "                              (lambda x,y:(x[0] + y[0], x[1] + y[1]))) # Whole RDD: --->(key,(TotalValueWholeRDD,TotalCountNumberWholeRDD))\n",
    "print(sumCount.collect())\n",
    "\n",
    "# mapAverage = sumCount.map(lambda key,xy:(key,xy[0]/xy[1])) \n",
    "# There is a Error when executed last one sentences,cased by Tuple Parameter Unpacking which has been removed in Python\n",
    "# the Error can be slove by the webpage : https://stackoverflow.com/questions/40207441/python-spark-combinebykey-average\n",
    "def pri(key_vals): \n",
    "    #print(\"Key is %s, Values is %d, Count is %d, Average is %f\" %(key,val[0],val[1],val[0]/val[1]))\n",
    "    (key,(total,count)) = key_vals\n",
    "    return key,(total/count)\n",
    "sumCount.map(pri).collect()\n",
    "\n",
    "#mapAverage.collectAsMap()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning the level of parallelism\n",
    "1. **Every RDD has a fixed number of partitions that determine the degree of parallelism to use when executing operations on the RDD.**\n",
    "2. When performing aggregations or grouping operations, we can ask spark to use a specific nunmber of Partitions\n",
    "3. Spark will always try to infer a sensible default value based on the size pf your cluster, but we can tune it to get better\n",
    "4. **All the functions in this chapter all can set *number of partions* as a parameter, Example is on the below:**\n",
    "```\n",
    "RDD.reduceByKey(lambda x,y: x + y)    # default Parallelism\n",
    "RDD.reduceByKey(lambda x,y: x + y,10) # set Parallelism by ourself\n",
    "```\n",
    "5. Sprak provide the *repartition()* function, which shuffles the data across the network to create a new set of partitions,**but it's a fairly *expensive* operation**, and Spark also provides an optimized version of the operation called *coalesce()*\n",
    "6. In Python we can use ** *RDD.getNumPartitions()* ** to make sure that we are coalescing it \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-04T12:17:12.734467Z",
     "start_time": "2017-11-04T12:17:12.726316Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sumCount.getNumPartitions()\n",
    "# run in local just has one partitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.2 Grouping Data\n",
    "1. With keyed data a common use case is grouping our data by key.\n",
    "2. If our data is already keyed in the way we want, groupByKey() will group our data using the key in our RDD. On an RDD consisting of keys of type K and values of type V, we get back an RDD of type [K, Iterable[V]].\n",
    "3. **groupBy() works on unpaired data or data where we want to use a different condition besides equality on the current key. It takes a function that it applies to every element in the source RDD and uses the result to determine the key.**\n",
    "4. In addition to grouping data from a single RDD, we can group data sharing the same key from multiple RDDs using a function called cogroup()\n",
    "5. ......"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.3 连接"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. jion() --> page:50\n",
    "2. leftOuterJoin() --> Page:51\n",
    "3. rightOuterJoin() --> Page:51"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-04T12:55:42.986953Z",
     "start_time": "2017-11-04T12:55:42.900933Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('foo', 1), ('kaka', 2), ('baz', 3), ('bar', 8)]\n",
      "[('foo', 4), ('bar', 5), ('bar', 6), ('Colin', 7)]\n"
     ]
    }
   ],
   "source": [
    "# Examples\n",
    "rdd1 =  sc.parallelize([(\"foo\", 1), (\"kaka\", 2), (\"baz\", 3),(\"bar\", 8)])\n",
    "rdd2 =  sc.parallelize([(\"foo\", 4), (\"bar\", 5), (\"bar\", 6),(\"Colin\",7)])\n",
    "\n",
    "print(rdd1.collect())\n",
    "print(rdd2.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-04T13:04:07.080184Z",
     "start_time": "2017-11-04T13:04:05.920696Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('foo', (1, 4)), ('bar', (8, 5)), ('bar', (8, 6))]\n",
      "[('foo', (1, 4)), ('kaka', (2, None)), ('baz', (3, None)), ('bar', (8, 5)), ('bar', (8, 6))]\n",
      "[('foo', (1, 4)), ('bar', (8, 5)), ('bar', (8, 6)), ('Colin', (None, 7))]\n"
     ]
    }
   ],
   "source": [
    "# join()只保留两个RDD都存在的键值对\n",
    "# 当某个键有多个值时，则生成的pair RDD会包括来自两个输入RDD的每一组相对于的记录\n",
    "rdd3 = rdd1.join(rdd2) \n",
    "print(rdd3.collect()) \n",
    "# leftOuterJoin()中，源RDD(rdd1)的所有键值都会被保存，每个value都是源RDD(rdd1)和rdd2的值的元组对，若rdd2中没有的键，值则为zone\n",
    "rdd4 = rdd1.leftOuterJoin(rdd2)\n",
    "print(rdd4.collect()) \n",
    "# rightOuterJoin()中，则以rdd2为中心，但是生成的value元组，***仍以rdd1的值为第一个元素***\n",
    "rdd5 = rdd1.rightOuterJoin(rdd2)\n",
    "print(rdd5.collect()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.4 数据排序\n",
    "1. sortByKey() --> page 51\n",
    "2. Exampel: 已字符串顺序对整数进行自定义排序\n",
    "\n",
    "`rdd.sortByKey(ascending=True, numPartition=None, keyfunc= lambda x: str(x))`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Pair RDD的行动操作\n",
    "1. page:52\n",
    "2. 常见行动操作：\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/ColinsGitCode/JupyterNotebook/acbfab7b376c184e490385919642dfcd3c7d46a1/ipynbFiles/Materials/%E5%B8%B8%E8%A7%81%E6%9C%89%E7%94%A8%E7%9A%84Pair_RDD%E8%A1%8C%E5%8A%A8%E6%93%8D%E4%BD%9C.jpg\" />"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "205px",
    "left": "576px",
    "right": "20px",
    "top": "112px",
    "width": "566px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
