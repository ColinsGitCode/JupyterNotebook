{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using mxnet backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np                                                                                                                                                                  \n",
    "from mxnet import nd, autograd\n",
    "import mxnet as mx\n",
    "from tensorly import backend as T\n",
    "from tensorly.tucker_tensor import tucker_to_tensor\n",
    "from tensorly.random import check_random_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tucker decomposition using SGD and autograd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We just fix the random seed for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "random_state = 1234\n",
    "rng = check_random_state(random_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a random tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape = [5, 5, 5]\n",
    "\n",
    "tensor = T.tensor(rng.random_sample(shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialise a random Tucker decomposition of that tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranks = [5, 5, 5]\n",
    "\n",
    "core = T.tensor(rng.random_sample(ranks))\n",
    "factors = [T.tensor(rng.random_sample((tensor.shape[i], ranks[i]))) for i in range(tensor.ndim)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is where the magic happens: we can attach gradients to the tensors!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "core.attach_grad()\n",
    "for f in factors:\n",
    "    f.attach_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use the simplest possible learning method: SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def SGD(params, lr):\n",
    "    for param in params:\n",
    "        param[:] = param - lr * param.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we just iterate through the training loop and backpropagate... \n",
    "\n",
    "You might have seen such loop in the Gluon tutorials -- if not, check them out! \n",
    "https://github.com/zackchase/mxnet-the-straight-dope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1000,. Rec. error: 0.347279884328\n",
      "Epoch 2000,. Rec. error: 0.278443665664\n",
      "Epoch 3000,. Rec. error: 0.247618793528\n",
      "Epoch 4000,. Rec. error: 0.225997005038\n",
      "Epoch 5000,. Rec. error: 0.210141680724\n",
      "Epoch 6000,. Rec. error: 0.199405929816\n",
      "Epoch 7000,. Rec. error: 0.191911998173\n",
      "Epoch 8000,. Rec. error: 0.185513944756\n",
      "Epoch 9000,. Rec. error: 0.179243332725\n",
      "Epoch 10000,. Rec. error: 0.172699486789\n",
      "Epoch 11000,. Rec. error: 0.165597723646\n",
      "Epoch 12000,. Rec. error: 0.157599644149\n",
      "Epoch 13000,. Rec. error: 0.148224587749\n",
      "Epoch 14000,. Rec. error: 0.136828436857\n",
      "Epoch 15000,. Rec. error: 0.122807824452\n",
      "Epoch 16000,. Rec. error: 0.106519893162\n",
      "Epoch 17000,. Rec. error: 0.0904362764887\n",
      "Epoch 18000,. Rec. error: 0.0770750921177\n",
      "Epoch 19000,. Rec. error: 0.0665911877544\n",
      "Epoch 20000,. Rec. error: 0.0581581382829\n",
      "Epoch 21000,. Rec. error: 0.0510929837756\n",
      "Epoch 22000,. Rec. error: 0.0450363835708\n",
      "Epoch 23000,. Rec. error: 0.0398456304225\n",
      "Epoch 24000,. Rec. error: 0.0354543330036\n",
      "Epoch 25000,. Rec. error: 0.031794251944\n",
      "Epoch 26000,. Rec. error: 0.0287780878427\n",
      "Epoch 27000,. Rec. error: 0.0263094838827\n",
      "Epoch 28000,. Rec. error: 0.024295418195\n",
      "Epoch 29000,. Rec. error: 0.0226531256596\n",
      "Epoch 30000,. Rec. error: 0.021312291481\n",
      "Epoch 31000,. Rec. error: 0.0202147078846\n",
      "Epoch 32000,. Rec. error: 0.0193128964442\n",
      "Epoch 33000,. Rec. error: 0.0185684851931\n",
      "Epoch 34000,. Rec. error: 0.017950634778\n",
      "Epoch 35000,. Rec. error: 0.0174346485257\n",
      "Epoch 36000,. Rec. error: 0.017000786486\n",
      "Epoch 37000,. Rec. error: 0.0166332867933\n",
      "Epoch 38000,. Rec. error: 0.0163195620246\n",
      "Epoch 39000,. Rec. error: 0.0160495560631\n"
     ]
    }
   ],
   "source": [
    "n_iter = 40000\n",
    "lr = 0.0005\n",
    "penalty = 0.1\n",
    "for i in range(1, n_iter):\n",
    "    \n",
    "    with autograd.record():\n",
    "        \n",
    "        # Reconstruct the tensor from the decomposed form\n",
    "        rec = tucker_to_tensor(core, factors)\n",
    "        \n",
    "        # l2 loss \n",
    "        loss = nd.sum((rec - tensor)**2)\n",
    "\n",
    "        # l2 penalty on the factors of the decomposition\n",
    "        for f in factors:\n",
    "            loss = loss + penalty * nd.sum(f**2)\n",
    "\n",
    "    loss.backward()\n",
    "    SGD([core] + factors, lr)\n",
    "\n",
    "    if i % 1000 == 0:\n",
    "        rec_error = T.norm(rec - tensor, 2)/T.norm(tensor, 2)\n",
    "        print(\"Epoch %s,. Rec. error: %s\" % (i, rec_error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
